{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EyeClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3)\n",
    "        self.fc1 = nn.Linear(512, 256)\n",
    "        self.fc2 = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "EyeClassifier(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# Load pre-trained face detector\n",
    "model_file = \"./Pretrained Detectors/res10_300x300_ssd_iter_140000.caffemodel\"\n",
    "config_file = \"./Pretrained Detectors/deploy.prototxt\"\n",
    "face_model = cv2.dnn.readNetFromCaffe(config_file, model_file)\n",
    "\n",
    "# Load pre-trained landmark predictor\n",
    "predictor = dlib.shape_predictor(\"./Pretrained Detectors/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# Load CNN eye classifier\n",
    "eye_model = EyeClassifier()\n",
    "eye_model.load_state_dict(torch.load(\"./Saved Models/model2.pt\"))\n",
    "eye_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locates bounding box for a single face\n",
    "def detect_face(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)     \n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(img, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "    face_model.setInput(blob)\n",
    "    detections = face_model.forward()\n",
    "\n",
    "    (x1, y1, x2, y2) = 0, 0, 0, 0\n",
    "    max_confidence = 0\n",
    "\n",
    "    for i in range(detections.shape[2]):                          \n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        if confidence > 0.5 and confidence > max_confidence:      # Only considers predictions with > 0.5 confidence\n",
    "            (h, w) = img.shape[:2]\n",
    "            x1 = int(detections[0, 0, i, 3] * w)\n",
    "            y1 = int(detections[0, 0, i, 4] * h)\n",
    "            x2 = int(detections[0, 0, i, 5] * w)\n",
    "            y2 = int(detections[0, 0, i, 6] * h)\n",
    "\n",
    "            max_confidence = confidence                           # If multiple faces are detected, only return the one with highest confidence\n",
    "\n",
    "    return dlib.rectangle(x1, y1, x2, y2), max_confidence\n",
    "\n",
    "\n",
    "# Locates bounding box for a single eye\n",
    "def detect_eye(img, face):\n",
    "    landmarks = predictor(img, face)\n",
    "\n",
    "    if landmarks.num_parts == 0:\n",
    "        return (0, 0, 0, 0), False\n",
    "    \n",
    "    \"\"\" Below is some random math I came up with to turn LEFT eye landmarks into a square box, feel free to change\"\"\"\n",
    "    x1 = landmarks.part(17).x                   \n",
    "    x2 = landmarks.part(21).x\n",
    "    d = abs(x2-x1)\n",
    "    k = d * 0.15\n",
    "\n",
    "    x1 = x1 - int(k/2)\n",
    "    x2 = x2 + int(k/2)\n",
    "    y1 = landmarks.part(19).y - int(k/2)\n",
    "    y2 = y1 + int(d+k)\n",
    "\n",
    "    return (x1, y1, x2, y2), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares an image for CNN eye classifier\n",
    "def preprocess(img):\n",
    "    t = transforms.Compose([transforms.Resize([24, 24]), \n",
    "                            transforms.ToTensor()]) \n",
    "                            \n",
    "    img = Image.fromarray(img).convert(\"L\")\n",
    "    img = ImageOps.equalize(img)\n",
    "    img = t(img)\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "# Predicts eye state given a single 1x24x24 tensor\n",
    "def predict_eye_state(img):\n",
    "    outputs = eye_model(img.unsqueeze(0))\n",
    "    prob = F.softmax(outputs, dim = 1)\n",
    "    pred = outputs.argmax(dim = 1).item()\n",
    "\n",
    "    #print(f\"Probabilities: ({prob[0][0]}, {prob[0][1]})\")\n",
    "    #print(\"Prediction:\", pred)\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(\"./Datasets/DROZY/videos_i8/1-1.mp4\")    \n",
    "#cap = cv2.VideoCapture(\"./Datasets/X/04/5.mp4\")   \n",
    "#cap = cv2.VideoCapture(0)   \n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()     # return status and image\n",
    "\n",
    "    if not ret:\n",
    "        print(\"Can't retreive frame\")\n",
    "        break\n",
    "\n",
    "    time.sleep(0.05)             # too see blinks clearly\n",
    "\n",
    "    # Face detection\n",
    "    face, confidence = detect_face(frame)\n",
    "    if confidence > 0:\n",
    "\n",
    "        # Eye detection\n",
    "        (x1, y1, x2, y2), eye_found = detect_eye(frame, face)\n",
    "\n",
    "        if eye_found:\n",
    "            # Eye state classification\n",
    "            eye = frame[y1:y2, x1:x2]\n",
    "            eye = preprocess(eye)\n",
    "            pred = predict_eye_state(eye)\n",
    "            \n",
    "\n",
    "            \"\"\" Do stuff with PERCLOS\"\"\"\n",
    "\n",
    "            if(pred == 0):\n",
    "                cv2.putText(frame, \"Closed\", (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "\n",
    "        # Rectangle visuals\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "        cv2.rectangle(frame, (face.left(), face.top()), (face.right(), face.bottom()), (255, 0, 0), 2)\n",
    "        confidence_txt = \"{:.2f}%\".format(confidence * 100)\n",
    "        cv2.putText(frame, confidence_txt, (face.left(), face.top()), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 1)\n",
    "\n",
    "\n",
    "    # Display frame\n",
    "    cv2.imshow(\"img\", frame)\n",
    "\n",
    "    # Exit window using \"q\" key\n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}